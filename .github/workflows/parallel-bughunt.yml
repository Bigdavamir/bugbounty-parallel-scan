name: Distributed BugHunt

on:
  workflow_dispatch:
    inputs:
      domain:
        description: "Target domain (e.g., example.com)"
        required: true
        type: string
      headers:
        description: 'Custom headers (optional, e.g., "Cookie: session=abc")'
        required: false
        type: string

jobs:
  # Job 1: Passive URL Collection
  wayback-collection:
    runs-on: ubuntu-latest
    timeout-minutes: 30
    outputs:
      wayback-count: ${{ steps.wayback.outputs.count }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Install waybackurls
        run: |
          WAYBACK_VER=$(curl -s https://api.github.com/repos/tomnomnom/waybackurls/releases/latest | grep tag_name | cut -d '"' -f4)
          curl -L "https://github.com/tomnomnom/waybackurls/releases/download/${WAYBACK_VER}/waybackurls_linux_amd64" -o waybackurls
          chmod +x waybackurls
          sudo mv waybackurls /usr/local/bin/

      - name: Run waybackurls
        id: wayback
        run: |
          mkdir -p wayback_results
          echo "${{ inputs.domain }}" | waybackurls > wayback_results/waybackurls.txt
          echo "count=$(wc -l < wayback_results/waybackurls.txt)" >> $GITHUB_OUTPUT

      - name: Upload wayback results
        uses: actions/upload-artifact@v4
        with:
          name: wayback-results
          path: wayback_results/

  # Job 2: GAU Collection (parallel)
  gau-collection:
    runs-on: ubuntu-latest
    timeout-minutes: 30
    outputs:
      gau-count: ${{ steps.gau.outputs.count }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Install gau
        run: |
          GAU_URL=$(curl -s https://api.github.com/repos/lc/gau/releases/latest | grep "browser_download_url.*linux_amd64" | grep -o 'https://[^"]*' | head -n1)
          wget -q -O gau "$GAU_URL"
          chmod +x gau
          sudo mv gau /usr/local/bin/

      - name: Run gau
        id: gau
        run: |
          mkdir -p gau_results
          gau "${{ inputs.domain }}" --threads 5 --subs > gau_results/gau.txt
          echo "count=$(wc -l < gau_results/gau.txt)" >> $GITHUB_OUTPUT

      - name: Upload gau results
        uses: actions/upload-artifact@v4
        with:
          name: gau-results
          path: gau_results/

  # Job 3: Combine URLs and prepare for distribution
  combine-and-prepare:
    runs-on: ubuntu-latest
    needs: [wayback-collection, gau-collection]
    timeout-minutes: 15
    outputs:
      total-urls: ${{ steps.combine.outputs.total }}
      dynamic-urls: ${{ steps.combine.outputs.dynamic }}
      param-count: ${{ steps.combine.outputs.params }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Install tools
        run: |
          sudo apt update && sudo apt install -y moreutils
          
          # anew
          ANEW_VER=$(curl -s https://api.github.com/repos/tomnomnom/anew/releases/latest | grep tag_name | cut -d '"' -f4)
          curl -L "https://github.com/tomnomnom/anew/releases/download/${ANEW_VER}/anew_linux_amd64" -o anew
          chmod +x anew && sudo mv anew /usr/local/bin/
          
          # unfurl
          UNFURL_VER=$(curl -s https://api.github.com/repos/tomnomnom/unfurl/releases/latest | grep tag_name | cut -d '"' -f4)
          curl -L "https://github.com/tomnomnom/unfurl/releases/download/${UNFURL_VER}/unfurl_linux_amd64" -o unfurl
          chmod +x unfurl && sudo mv unfurl /usr/local/bin/

      - name: Download results
        uses: actions/download-artifact@v4

      - name: Combine and filter URLs
        id: combine
        run: |
          mkdir -p combined_results
          
          # Combine all URLs
          cat wayback-results/waybackurls.txt gau-results/gau.txt > all-raw-urls.txt
          echo "https://${{ inputs.domain }}/" >> all-raw-urls.txt
          
          # Filter extensions and clean
          EXTS='(json|js|css|jpg|jpeg|png|svg|gif|exe|mp4|flv|pdf|doc|webm|wmv|webp|mov|mp3|avi|zip)($|\?)'
          grep -iEv "\.${EXTS}" all-raw-urls.txt | awk 'NF' | anew all-clean-urls.txt
          
          # Separate static and dynamic URLs
          grep -v '?' all-clean-urls.txt > static-urls.txt || touch static-urls.txt
          grep '?' all-clean-urls.txt > dynamic-urls.txt || touch dynamic-urls.txt
          
          # Extract parameters
          unfurl --unique keys < dynamic-urls.txt | sort -u > params.txt || touch params.txt
          
          # Output counts
          echo "total=$(wc -l < all-clean-urls.txt)" >> $GITHUB_OUTPUT
          echo "dynamic=$(wc -l < dynamic-urls.txt)" >> $GITHUB_OUTPUT  
          echo "params=$(wc -l < params.txt)" >> $GITHUB_OUTPUT
          
          # Split dynamic URLs for parallel processing (chunks of 200)
          if [ -s dynamic-urls.txt ]; then
            split -l 200 dynamic-urls.txt combined_results/chunk_
          fi

      - name: Upload combined results
        uses: actions/upload-artifact@v4
        with:
          name: combined-results
          path: |
            all-raw-urls.txt
            all-clean-urls.txt
            static-urls.txt
            dynamic-urls.txt
            params.txt
            combined_results/

  # Job 4: Parallel httpx scanning (Matrix Strategy)
  httpx-scan:
    runs-on: ubuntu-latest
    needs: combine-and-prepare
    if: needs.combine-and-prepare.outputs.dynamic-urls != '0'
    timeout-minutes: 60
    strategy:
      matrix:
        worker: [1, 2, 3, 4, 5, 6]  # 6 parallel workers
      fail-fast: false
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Install httpx
        run: |
          HTTPX_VER=$(curl -s https://api.github.com/repos/projectdiscovery/httpx/releases/latest | grep tag_name | cut -d '"' -f4)
          curl -L "https://github.com/projectdiscovery/httpx/releases/download/${HTTPX_VER}/httpx_${HTTPX_VER}_linux_amd64.tar.gz" -o httpx.tar.gz
          tar -xzf httpx.tar.gz httpx
          sudo mv httpx /usr/local/bin/

      - name: Download URL chunks
        uses: actions/download-artifact@v4
        with:
          name: combined-results

      - name: Process URLs with httpx
        run: |
          mkdir -p httpx_results
          
          # Find chunk files for this worker
          CHUNK_FILES=(combined_results/chunk_*)
          TOTAL_CHUNKS=${#CHUNK_FILES[@]}
          
          if [ $TOTAL_CHUNKS -gt 0 ]; then
            # Calculate which chunks this worker should process
            WORKER_ID=${{ matrix.worker }}
            
            for ((i=WORKER_ID-1; i<TOTAL_CHUNKS; i+=6)); do
              CHUNK_FILE="${CHUNK_FILES[$i]}"
              if [ -f "$CHUNK_FILE" ]; then
                echo "Worker $WORKER_ID processing: $CHUNK_FILE"
                
                # Run httpx with timeout
                timeout 180 httpx -silent -threads 50 -timeout 8 -retries 2 \
                  < "$CHUNK_FILE" \
                  > "httpx_results/worker_${WORKER_ID}_chunk_${i}.txt" || true
              fi
            done
          fi

      - name: Upload httpx results
        uses: actions/upload-artifact@v4
        with:
          name: httpx-results-worker-${{ matrix.worker }}
          path: httpx_results/

  # Job 5: x8 Parameter Discovery (Parallel)
  x8-scan:
    runs-on: ubuntu-latest
    needs: [combine-and-prepare, httpx-scan]
    if: always() && needs.combine-and-prepare.outputs.dynamic-urls != '0'
    timeout-minutes: 90
    strategy:
      matrix:
        worker: [1, 2, 3]  # 3 parallel workers for x8
      fail-fast: false
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Install x8
        run: |
          X8_VER=$(curl -s https://api.github.com/repos/Sh1Yo/x8/releases/latest | grep tag_name | cut -d '"' -f4)
          curl -L "https://github.com/Sh1Yo/x8/releases/download/${X8_VER}/x8_linux_amd64" -o x8
          chmod +x x8 && sudo mv x8 /usr/local/bin/

      - name: Download all httpx results and params
        uses: actions/download-artifact@v4

      - name: Combine httpx results and run x8
        run: |
          mkdir -p x8_results
          
          # Combine all httpx results
          find . -name "httpx-results-worker-*" -type d -exec cat {}/*.txt \; > all-alive-urls.txt
          
          # Filter only dynamic URLs
          grep '?' all-alive-urls.txt > dynamic-alive.txt || touch dynamic-alive.txt
          
          if [ -s dynamic-alive.txt ] && [ -s combined-results/params.txt ]; then
            # Split URLs for this worker
            WORKER_ID=${{ matrix.worker }}
            split -l 50 dynamic-alive.txt x8_chunk_
            
            X8_CHUNKS=(x8_chunk_*)
            TOTAL_X8_CHUNKS=${#X8_CHUNKS[@]}
            
            # Process chunks assigned to this worker
            for ((i=WORKER_ID-1; i<TOTAL_X8_CHUNKS; i+=3)); do
              CHUNK_FILE="${X8_CHUNKS[$i]}"
              if [ -f "$CHUNK_FILE" ]; then
                echo "X8 Worker $WORKER_ID processing: $CHUNK_FILE"
                
                while IFS= read -r url; do
                  echo "Processing URL: $url"
                  X8_CMD="x8 -u "$url" -w "combined-results/params.txt" -X GET POST"
                  
                  if [ -n "${{ inputs.headers }}" ]; then
                    X8_CMD+=" -H "${{ inputs.headers }}""
                  fi
                  
                  timeout 120 bash -c "$X8_CMD" >> "x8_results/worker_${WORKER_ID}_${i}.txt" 2>/dev/null || true
                done < "$CHUNK_FILE"
              fi
            done
          fi

      - name: Upload x8 results
        uses: actions/upload-artifact@v4
        with:
          name: x8-results-worker-${{ matrix.worker }}
          path: x8_results/

  # Job 6: kxss Scanning (Parallel)
  kxss-scan:
    runs-on: ubuntu-latest
    needs: [combine-and-prepare, httpx-scan]
    if: always() && needs.combine-and-prepare.outputs.dynamic-urls != '0'
    timeout-minutes: 120
    strategy:
      matrix:
        worker: [1, 2, 3, 4]  # 4 parallel workers for kxss
      fail-fast: false
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Install kxss
        run: |
          KXSS_VER=$(curl -s https://api.github.com/repos/tomnomnom/kxss/releases/latest | grep tag_name | cut -d '"' -f4)
          curl -L "https://github.com/tomnomnom/kxss/releases/download/${KXSS_VER}/kxss_linux_amd64" -o kxss
          chmod +x kxss && sudo mv kxss /usr/local/bin/

      - name: Download results
        uses: actions/download-artifact@v4

      - name: Generate kxss URLs and scan
        run: |
          mkdir -p kxss_results
          
          # Combine httpx results
          find . -name "httpx-results-worker-*" -type d -exec cat {}/*.txt \; > all-alive-urls.txt
          grep '?' all-alive-urls.txt > dynamic-alive.txt || touch dynamic-alive.txt
          
          if [ -s dynamic-alive.txt ] && [ -s combined-results/params.txt ]; then
            # Generate kxss URLs
            > kxss_urls.txt
            echo "https://1.bigdav.ir/test.php?test=KXSS" >> kxss_urls.txt  # Health check
            
            while IFS= read -r url; do
              while IFS= read -r param; do
                if [[ "$url" == *"?${param}="* ]]; then
                  echo "$url" | sed "s/\([\?&]${param}=\)[^&]*/\1KXSS/" >> kxss_urls.txt
                elif [[ "$url" == *\?* ]]; then
                  echo "${url}&${param}=KXSS" >> kxss_urls.txt
                else
                  echo "${url}?${param}=KXSS" >> kxss_urls.txt
                fi
              done < combined-results/params.txt
            done < dynamic-alive.txt
            
            # Remove duplicates and split for workers
            sort -u kxss_urls.txt > kxss_unique.txt
            split -l 200 kxss_unique.txt kxss_chunk_
            
            # Process chunks for this worker
            WORKER_ID=${{ matrix.worker }}
            KXSS_CHUNKS=(kxss_chunk_*)
            TOTAL_KXSS_CHUNKS=${#KXSS_CHUNKS[@]}
            
            for ((i=WORKER_ID-1; i<TOTAL_KXSS_CHUNKS; i+=4)); do
              CHUNK_FILE="${KXSS_CHUNKS[$i]}"
              if [ -f "$CHUNK_FILE" ]; then
                echo "KXSS Worker $WORKER_ID processing: $CHUNK_FILE"
                timeout 600 kxss --timeout 15 < "$CHUNK_FILE" > "kxss_results/worker_${WORKER_ID}_${i}.txt" 2>/dev/null || true
              fi
            done
          fi

      - name: Upload kxss results
        uses: actions/upload-artifact@v4
        with:
          name: kxss-results-worker-${{ matrix.worker }}
          path: kxss_results/

  # Job 7: Final Summary and Report
  final-summary:
    runs-on: ubuntu-latest
    needs: [wayback-collection, gau-collection, combine-and-prepare, httpx-scan, x8-scan, kxss-scan]
    if: always()
    timeout-minutes: 15
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Download all results
        uses: actions/download-artifact@v4

      - name: Generate final summary
        run: |
          mkdir -p final_results
          
          # Combine all httpx results
          find . -name "httpx-results-worker-*" -type d -exec cat {}/*.txt \; > all-alive-urls.txt
          
          # Combine all x8 results  
          find . -name "x8-results-worker-*" -type d -exec cat {}/*.txt \; > all-x8-reflections.txt
          
          # Combine all kxss results
          find . -name "kxss-results-worker-*" -type d -exec cat {}/*.txt \; > all-kxss-reflections.txt
          
          # Parse kxss reflections
          awk '
          /^URL: .* Param: .* Unfiltered: / {
            url_start = index($0, "URL: ") + 5
            param_pos = index($0, " Param: ")
            url = substr($0, url_start, param_pos - url_start)
            
            param_start = param_pos + 8
            unfilt_pos = index($0, " Unfiltered: ")
            param = substr($0, param_start, unfilt_pos - param_start)
            
            unfilt_start = unfilt_pos + 13
            unfiltered = substr($0, unfilt_start)
            
            if (unfiltered != "[]" && unfiltered != "" && url != "" && param != "") {
              print url " | " param " | Unfiltered: " unfiltered
            }
          }
          ' all-kxss-reflections.txt > kxss-parsed-reflections.txt
          
          # Generate summary report
          cat > FINAL_SUMMARY.txt << EOF
          ============ BugHunt Summary for ${{ inputs.domain }} ============
          Wayback URLs: ${{ needs.wayback-collection.outputs.wayback-count }}
          GAU URLs: ${{ needs.gau-collection.outputs.gau-count }}
          Total Clean URLs: ${{ needs.combine-and-prepare.outputs.total-urls }}
          Dynamic URLs: ${{ needs.combine-and-prepare.outputs.dynamic-urls }}
          Unique Parameters: ${{ needs.combine-and-prepare.outputs.param-count }}
          Alive URLs (httpx): $(wc -l < all-alive-urls.txt 2>/dev/null || echo 0)
          X8 Reflection Lines: $(wc -l < all-x8-reflections.txt 2>/dev/null || echo 0) 
          KXSS Raw Lines: $(wc -l < all-kxss-reflections.txt 2>/dev/null || echo 0)
          KXSS Parsed Reflections: $(wc -l < kxss-parsed-reflections.txt 2>/dev/null || echo 0)
          Headers Used: ${{ inputs.headers }}
          ==============================================================
          EOF
          
          # Show parsed reflections if any
          if [ -s kxss-parsed-reflections.txt ]; then
            echo "" >> FINAL_SUMMARY.txt
            echo "=== KXSS Reflections Found ===" >> FINAL_SUMMARY.txt
            head -20 kxss-parsed-reflections.txt >> FINAL_SUMMARY.txt
          fi
          
          # Health check
          if grep -q "1.bigdav.ir" kxss-parsed-reflections.txt; then
            echo "✓ Health check PASSED" >> FINAL_SUMMARY.txt
          else
            echo "✗ Health check FAILED" >> FINAL_SUMMARY.txt
          fi
          
          cat FINAL_SUMMARY.txt

      - name: Upload final results
        uses: actions/upload-artifact@v4
        with:
          name: final-results
          path: |
            all-alive-urls.txt
            all-x8-reflections.txt
            all-kxss-reflections.txt
            kxss-parsed-reflections.txt
            FINAL_SUMMARY.txt
