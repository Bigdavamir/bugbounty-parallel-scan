name: Distributed BugHunt - Exact Flow

on:
  workflow_dispatch:
    inputs:
      domain:
        description: "Target domain (e.g., example.com)"
        required: true
        type: string
      headers:
        description: 'Custom headers (optional, e.g., "Cookie: session=abc")'
        required: false
        type: string

jobs:
  # Job 1: waybackurls + gau (parallel)
  passive-collection:
    runs-on: ubuntu-latest
    timeout-minutes: 30
    strategy:
      matrix:
        tool: [wayback, gau]
      fail-fast: false
    outputs:
      wayback-done: ${{ steps.wayback.outputs.done }}
      gau-done: ${{ steps.gau.outputs.done }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Go
        uses: actions/setup-go@v4
        with:
          go-version: '1.21'
          cache: false

      - name: Install tools
        run: |
          go install github.com/tomnomnom/waybackurls@latest
          go install github.com/lc/gau/v2/cmd/gau@latest
          go install github.com/tomnomnom/anew@latest

      - name: Run waybackurls
        if: matrix.tool == 'wayback'
        id: wayback
        run: |
          mkdir -p results
          echo "https://${{ inputs.domain }}/" > results/all-urls.tmp
          echo "${{ inputs.domain }}" | waybackurls | tee results/waybackurls.txt >> results/all-urls.tmp
          echo "done=true" >> $GITHUB_OUTPUT

      - name: Run gau
        if: matrix.tool == 'gau'
        id: gau
        run: |
          mkdir -p results
          gau "${{ inputs.domain }}" --threads 5 --subs | tee results/gau.txt
          echo "done=true" >> $GITHUB_OUTPUT

      - name: Upload results
        uses: actions/upload-artifact@v4
        with:
          name: passive-${{ matrix.tool }}
          path: results/

  # Job 2: Combine and filter URLs (exact same logic)
  combine-and-filter:
    runs-on: ubuntu-latest
    needs: passive-collection
    timeout-minutes: 15
    outputs:
      dynamic-count: ${{ steps.filter.outputs.dynamic }}
      params-count: ${{ steps.filter.outputs.params }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Go
        uses: actions/setup-go@v4
        with:
          go-version: '1.21'
          cache: false

      - name: Install tools
        run: |
          go install github.com/tomnomnom/anew@latest
          go install github.com/tomnomnom/unfurl@latest

      - name: Download results
        uses: actions/download-artifact@v4

      - name: Combine and filter (exact main.sh logic)
        id: filter
        run: |
          # Extensions to filter out (exact from main.sh)
          EXTS='(json|js|css|jpg|jpeg|png|svg|gif|exe|mp4|flv|pdf|doc|webm|wmv|webp|mov|mp3|avi|zip)($|\?)'
          
          # Combine all URLs
          cat passive-wayback/waybackurls.txt passive-gau/gau.txt passive-wayback/all-urls.tmp > all-urls.txt
          sort -u all-urls.txt > all-urls-unique.txt
          
          # Filter extensions and clean (exact same as main.sh)
          cat all-urls-unique.txt \
            | grep -iEv "\.${EXTS}" \
            | awk 'NF' \
            | anew "${{ inputs.domain }}.passive"
          
          # Extract unique parameter keys with unfurl (exact same)
          unfurl --unique keys < "${{ inputs.domain }}.passive" \
            | anew unfurl-params.txt \
            | sort -u > unfurl-params.tmp
          mv unfurl-params.tmp unfurl-params.txt
          
          # Output counts
          echo "dynamic=$(grep '?' "${{ inputs.domain }}.passive" | wc -l)" >> $GITHUB_OUTPUT
          echo "params=$(wc -l < unfurl-params.txt)" >> $GITHUB_OUTPUT

      - name: Upload combined results
        uses: actions/upload-artifact@v4
        with:
          name: combined-results
          path: |
            ${{ inputs.domain }}.passive
            unfurl-params.txt
            all-urls.txt

  # Job 3: httpx in chunks (parallel - exact logic)
  httpx-scan:
    runs-on: ubuntu-latest
    needs: combine-and-filter
    if: needs.combine-and-filter.outputs.dynamic-count != '0'
    timeout-minutes: 45
    strategy:
      matrix:
        chunk: [1, 2, 3, 4, 5, 6, 7, 8]  # 8 parallel chunks
      fail-fast: false
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Go
        uses: actions/setup-go@v4
        with:
          go-version: '1.21'
          cache: false

      - name: Install tools
        run: |
          go install github.com/projectdiscovery/httpx/cmd/httpx@latest
          go install github.com/tomnomnom/anew@latest

      - name: Download URLs
        uses: actions/download-artifact@v4
        with:
          name: combined-results

      - name: Run httpx on chunk (exact main.sh logic)
        run: |
          mkdir -p httpx_results
          
          # Create dynamic URLs temp file (exact same as main.sh)
          grep '?' "${{ inputs.domain }}.passive" | sort -u > dynamic_passive.tmp
          
          # Split into chunks (150 per chunk like main.sh)
          split -l 150 dynamic_passive.tmp chunk_
          
          # Get chunk files
          CHUNK_FILES=(chunk_*)
          TOTAL_CHUNKS=${#CHUNK_FILES[@]}
          
          # Calculate which chunk this worker should process
          WORKER_ID=${{ matrix.chunk }}
          
          if [ $WORKER_ID -le $TOTAL_CHUNKS ]; then
            CHUNK_FILE="chunk_$(printf "%02d" $((WORKER_ID-1)))"
            if [ -f "$CHUNK_FILE" ]; then
              echo "Processing chunk: $CHUNK_FILE"
              # Exact same httpx command as main.sh
              httpx -silent -threads 100 -timeout 2 -retries 2 < "$CHUNK_FILE" > "httpx_results/chunk_${WORKER_ID}.httpx"
            fi
          fi
          
          rm -f chunk_* dynamic_passive.tmp

      - name: Upload httpx results
        uses: actions/upload-artifact@v4
        with:
          name: httpx-results-${{ matrix.chunk }}
          path: httpx_results/

  # Job 4: x8 scan (parallel - exact logic)
  x8-scan:
    runs-on: ubuntu-latest
    needs: [combine-and-filter, httpx-scan]
    if: always() && needs.combine-and-filter.outputs.dynamic-count != '0'
    timeout-minutes: 60
    strategy:
      matrix:
        worker: [1, 2, 3, 4, 5, 6, 7, 8]  # 8 parallel workers
      fail-fast: false
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Install x8
        run: |
          curl -L "https://github.com/Sh1Yo/x8/releases/latest/download/x8-linux-x86-64" -o x8
          chmod +x x8
          sudo mv x8 /usr/local/bin/

      - name: Download results
        uses: actions/download-artifact@v4

      - name: Combine httpx and run x8 (exact main.sh logic)
        run: |
          mkdir -p x8_results
          
          # Combine all httpx results (exact same as main.sh)
          find . -name "httpx-results-*" -type d -exec cat {}/*.httpx \; > httpx.txt || true
          find . -name "httpx-results-*" -type d -exec grep '?' {}/*.httpx \; > dynamic-httpx.txt || true
          
          # x8 command exactly like main.sh
          X8_CMD='x8 -u "{}" -w "'"$PWD/combined-results/unfurl-params.txt"'" -X GET POST'
          if [[ -n "${{ inputs.headers }}" ]]; then
            X8_CMD+=' -H "${{ inputs.headers }}"'
          fi
          
          # Split dynamic URLs for parallel processing
          if [ -s dynamic-httpx.txt ]; then
            TOTAL_URLS=$(wc -l < dynamic-httpx.txt)
            URLS_PER_WORKER=$((TOTAL_URLS / 8 + 1))
            WORKER_ID=${{ matrix.worker }}
            START_LINE=$(((WORKER_ID-1) * URLS_PER_WORKER + 1))
            END_LINE=$((WORKER_ID * URLS_PER_WORKER))
            
            sed -n "${START_LINE},${END_LINE}p" dynamic-httpx.txt | \
              xargs -P 1 -I{} bash -c "$X8_CMD" > "x8_results/worker_${WORKER_ID}.txt" || true
          fi

      - name: Upload x8 results
        uses: actions/upload-artifact@v4
        with:
          name: x8-results-${{ matrix.worker }}
          path: x8_results/

  # Job 5: Generate kxss URLs (exact main.sh logic)
  generate-kxss-urls:
    runs-on: ubuntu-latest
    needs: [combine-and-filter, httpx-scan]
    if: always() && needs.combine-and-filter.outputs.dynamic-count != '0'
    timeout-minutes: 30
    outputs:
      kxss-urls-count: ${{ steps.generate.outputs.count }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Download results
        uses: actions/download-artifact@v4

      - name: Generate kxss URLs (exact main.sh logic)
        id: generate
        run: |
          mkdir -p kxss_scan
          
          # Combine httpx results
          find . -name "httpx-results-*" -type d -exec grep '?' {}/*.httpx \; > dynamic-httpx.txt || true
          
          # Exact same function from main.sh
          gen_single_param_url() {
            local base="$1"
            local param="$2"
            
            if [[ "$base" == *"?${param}="* ]]; then
              echo "$base" | sed "s/\([\?&]${param}=\)[^&]*/\1KXSS/"
            elif [[ "$base" == *\?* ]]; then
              echo "${base}&${param}=KXSS"
            else
              echo "${base}?${param}=KXSS"
            fi
          }
          
          # Generate URLs exactly like main.sh
          url_param_log="kxss_scan/urls_with_params.log"
          : > "$url_param_log"
          
          # Always include healthcheck payload first
          echo "https://1.bigdav.ir/test.php?test=KXSS" >> "$url_param_log"
          
          if [[ -s combined-results/unfurl-params.txt ]] && [[ -s dynamic-httpx.txt ]]; then
            mapfile -t params < <(sort -u combined-results/unfurl-params.txt)
            mapfile -t urls < <(sort -u dynamic-httpx.txt)
            
            # Generate each parameter separately for each URL (exact same)
            for u in "${urls[@]}"; do
              for p in "${params[@]}"; do
                gen_single_param_url "$u" "$p" >> "$url_param_log"
              done
            done
          fi
          
          # Remove duplicates and save
          sort -u "$url_param_log" > "kxss_scan/kxss_urls.txt"
          echo "count=$(wc -l < kxss_scan/kxss_urls.txt)" >> $GITHUB_OUTPUT

      - name: Upload kxss URLs
        uses: actions/upload-artifact@v4
        with:
          name: kxss-urls
          path: kxss_scan/

  # Job 6: kxss scan (parallel - exact logic)
  kxss-scan:
    runs-on: ubuntu-latest
    needs: generate-kxss-urls
    if: needs.generate-kxss-urls.outputs.kxss-urls-count != '0'
    timeout-minutes: 90
    strategy:
      matrix:
        worker: [1, 2, 3, 4, 5, 6]  # 6 parallel workers
      fail-fast: false
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Install kxss
        run: |
          curl -L "https://github.com/tomnomnom/kxss/releases/latest/download/kxss-linux-amd64" -o kxss
          chmod +x kxss
          sudo mv kxss /usr/local/bin/

      - name: Download kxss URLs
        uses: actions/download-artifact@v4
        with:
          name: kxss-urls

      - name: Run kxss on chunk (exact main.sh logic)
        run: |
          mkdir -p kxss_results
          
          # Split URLs into chunks (100 per chunk like main.sh)
          CHUNK_SIZE=100
          split -l "$CHUNK_SIZE" "kxss_scan/kxss_urls.txt" "chunk_"
          
          # Get chunk files for this worker
          CHUNK_FILES=(chunk_*)
          TOTAL_CHUNKS=${#CHUNK_FILES[@]}
          WORKER_ID=${{ matrix.worker }}
          
          # Process chunks assigned to this worker
          for ((i=WORKER_ID-1; i<TOTAL_CHUNKS; i+=6)); do
            CHUNK_FILE="${CHUNK_FILES[$i]}"
            if [ -f "$CHUNK_FILE" ]; then
              echo "Worker $WORKER_ID processing: $CHUNK_FILE"
              timeout 300 kxss < "$CHUNK_FILE" > "kxss_results/worker_${WORKER_ID}_${i}.txt" || true
            fi
          done

      - name: Upload kxss results
        uses: actions/upload-artifact@v4
        with:
          name: kxss-results-${{ matrix.worker }}
          path: kxss_results/

  # Job 7: Final summary (exact main.sh logic)
  final-summary:
    runs-on: ubuntu-latest
    needs: [passive-collection, combine-and-filter, httpx-scan, x8-scan, kxss-scan]
    if: always()
    timeout-minutes: 15
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Download all results
        uses: actions/download-artifact@v4

      - name: Generate final summary (exact main.sh logic)
        run: |
          # Combine all results exactly like main.sh
          find . -name "httpx-results-*" -type d -exec cat {}/*.httpx \; > httpx.txt || true
          find . -name "httpx-results-*" -type d -exec grep '?' {}/*.httpx \; > dynamic-httpx.txt || true
          find . -name "x8-results-*" -type d -exec cat {}/*.txt \; > x8-brute.txt || true
          find . -name "kxss-results-*" -type d -exec cat {}/*.txt \; > kxss-out.txt || true
          
          # Parse kxss output exactly like main.sh
          awk '
          /^URL: .* Param: .* Unfiltered: / {
            url_start = index($0, "URL: ") + 5
            param_pos = index($0, " Param: ")
            url = substr($0, url_start, param_pos - url_start)
            
            param_start = param_pos + 8
            unfilt_pos = index($0, " Unfiltered: ")
            param = substr($0, param_start, unfilt_pos - param_start)
            
            unfilt_start = unfilt_pos + 13
            unfiltered = substr($0, unfilt_start)
            
            if (unfiltered != "[]" && unfiltered != "" && url != "" && param != "") {
              print url " | " param " | Unfiltered: " unfiltered
            }
          }
          ' kxss-out.txt > kxss-reflected-pairs.txt
          
          # Count functions exactly like main.sh
          count_or_zero(){
            [[ -f "$1" ]] && wc -l < "$1" || echo 0
          }
          
          # Generate summary exactly like main.sh
          WAYBACK_COUNT=$(count_or_zero passive-wayback/waybackurls.txt)
          GAU_COUNT=$(count_or_zero passive-gau/gau.txt)
          ALLURLS_COUNT=$(count_or_zero combined-results/all-urls.txt)
          STATIC_COUNT=$(grep -iv "?" combined-results/all-urls.txt 2>/dev/null | sort -u | wc -l || echo 0)
          DYNAMIC_COUNT=$(count_or_zero dynamic-httpx.txt)
          HTTPX_COUNT=$(count_or_zero httpx.txt)
          UNFURLPARAMS_COUNT=$(count_or_zero combined-results/unfurl-params.txt)
          X8_COUNT=$(count_or_zero x8-brute.txt)
          KXSS_COUNT=$(count_or_zero kxss-out.txt)
          
          echo "============ Recon Summary for ${{ inputs.domain }} ============"
          printf "%-22s: %d
" "waybackurls" "$WAYBACK_COUNT"
          printf "%-22s: %d
" "gau" "$GAU_COUNT"
          printf "%-22s: %d
" "All unique URLs" "$ALLURLS_COUNT"
          printf "%-22s: %d
" "Static URLs" "$STATIC_COUNT"
          printf "%-22s: %d
" "Dynamic URLs" "$DYNAMIC_COUNT"
          printf "%-22s: %d
" "Unique URL params" "$UNFURLPARAMS_COUNT"
          printf "%-22s: %d
" "httpx (alive URLs)" "$HTTPX_COUNT"
          printf "%-22s: %d
" "x8 reflections lines" "$X8_COUNT"
          printf "%-22s: %d
" "kxss scan lines" "$KXSS_COUNT"
          [[ -n "${{ inputs.headers }}" ]] && printf "%-22s: %s\n" "Headers used" "${{ inputs.headers }}"
          echo "=================================================="
          
          # Show reflected pairs exactly like main.sh
          if [[ -s "kxss-reflected-pairs.txt" ]]; then
            echo "[*] First 5 reflected pairs:"
            head -5 "kxss-reflected-pairs.txt"
          else
            echo "[!] No reflected pairs found."
          fi
          
          # Health check exactly like main.sh
          if grep -q "1.bigdav.ir" "kxss-reflected-pairs.txt"; then
            echo "[✓] Health check passed."
          else
            echo "[✗] Health check failed."
          fi

      - name: Upload final results
        uses: actions/upload-artifact@v4
        with:
          name: final-results
          path: |
            httpx.txt
            dynamic-httpx.txt
            x8-brute.txt
            kxss-out.txt
            kxss-reflected-pairs.txt
