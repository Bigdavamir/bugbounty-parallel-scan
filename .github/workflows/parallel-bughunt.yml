name: Advanced Distributed BugHunt - Fixed

on:
  workflow_dispatch:
    inputs:
      domain:
        description: "Target domain (e.g., example.com)"
        required: true

jobs:
  # Job 1: waybackurls only
  wayback-collection:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Install basic tools
        run: |
          sudo apt update
          sudo apt install -y curl wget jq

      - name: Install waybackurls (Alternative method)
        run: |
          # Method 1: Try direct binary download
          echo "Trying direct binary download..."
          ARCH="linux-amd64"
          
          # Get latest release info
          LATEST=$(curl -s https://api.github.com/repos/tomnomnom/waybackurls/releases/latest)
          VERSION=$(echo "$LATEST" | jq -r '.tag_name')
          
          echo "Found version: $VERSION"
          
          # Try different download patterns
          URLS=(
            "https://github.com/tomnomnom/waybackurls/releases/download/${VERSION}/waybackurls-${ARCH}-${VERSION}.tgz"
            "https://github.com/tomnomnom/waybackurls/releases/download/${VERSION}/waybackurls_${ARCH}"
            "https://github.com/tomnomnom/waybackurls/releases/download/${VERSION}/waybackurls_linux_amd64"
          )
          
          INSTALLED=false
          for url in "${URLS[@]}"; do
            echo "Trying: $url"
            if wget -q "$url" -O waybackurls_download 2>/dev/null; then
              echo "Downloaded successfully from: $url"
              
              # Check if it's a tar file
              if file waybackurls_download | grep -q "gzip"; then
                echo "Extracting tar.gz file..."
                tar -xzf waybackurls_download
                find . -name "waybackurls" -type f -executable | head -1 | xargs -I{} cp {} waybackurls_binary
              else
                echo "Using direct binary..."
                cp waybackurls_download waybackurls_binary
              fi
              
              chmod +x waybackurls_binary
              sudo mv waybackurls_binary /usr/local/bin/waybackurls
              
              # Test installation
              if waybackurls --help >/dev/null 2>&1 || waybackurls -h >/dev/null 2>&1; then
                echo "waybackurls installed successfully"
                INSTALLED=true
                break
              fi
            fi
          done
          
          # Fallback: Install Go and build from source
          if [ "$INSTALLED" = false ]; then
            echo "Binary installation failed, trying Go installation..."
            sudo apt install -y golang-go
            export PATH=$PATH:/usr/local/go/bin:~/go/bin
            go install github.com/tomnomnom/waybackurls@latest
            sudo cp ~/go/bin/waybackurls /usr/local/bin/ 2>/dev/null || echo "waybackurls available in ~/go/bin/"
            export PATH="$HOME/go/bin:$PATH"
            echo "$HOME/go/bin" >> $GITHUB_PATH
          fi

      - name: Run waybackurls
        run: |
          DOMAIN="${{ github.event.inputs.domain }}"
          mkdir -p wayback_results
          
          echo "Testing waybackurls installation..."
          which waybackurls || echo "waybackurls not in PATH"
          
          # Add go bin to PATH if needed
          export PATH="$HOME/go/bin:/usr/local/bin:$PATH"
          
          echo "Running waybackurls for: $DOMAIN"
          
          # Try different approaches
          if command -v waybackurls >/dev/null 2>&1; then
            echo "$DOMAIN" | waybackurls > wayback_results/waybackurls.txt 2>&1 || {
              echo "waybackurls failed, creating empty file"
              touch wayback_results/waybackurls.txt
            }
          else
            echo "waybackurls not found, creating empty file"
            touch wayback_results/waybackurls.txt
          fi
          
          WAYBACK_COUNT=$(wc -l < wayback_results/waybackurls.txt)
          echo "Waybackurls found $WAYBACK_COUNT URLs"
          
          # Add some manual URLs as fallback
          echo "https://${DOMAIN}/" >> wayback_results/waybackurls.txt
          echo "https://www.${DOMAIN}/" >> wayback_results/waybackurls.txt
          echo "https://${DOMAIN}/admin" >> wayback_results/waybackurls.txt
          echo "https://${DOMAIN}/login" >> wayback_results/waybackurls.txt
          echo "https://${DOMAIN}/api" >> wayback_results/waybackurls.txt

      - name: Upload waybackurls results
        uses: actions/upload-artifact@v4
        with:
          name: wayback-results
          path: wayback_results/

  # Job 2: gau collection
  gau-collection:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Install Go and gau
        run: |
          # Install Go
          sudo apt update
          sudo apt install -y golang-go curl
          
          # Set up Go environment
          export GOPATH=$HOME/go
          export PATH=$PATH:/usr/local/go/bin:$GOPATH/bin
          echo "export PATH=\$PATH:/usr/local/go/bin:\$HOME/go/bin" >> ~/.bashrc
          
          # Install gau
          echo "Installing gau..."
          go install github.com/lc/gau/v2/cmd/gau@latest
          
          # Copy to system bin
          sudo cp $HOME/go/bin/gau /usr/local/bin/ 2>/dev/null || echo "gau available in ~/go/bin/"
          
          # Add to PATH for this session
          echo "$HOME/go/bin" >> $GITHUB_PATH

      - name: Run gau
        run: |
          DOMAIN="${{ github.event.inputs.domain }}"
          mkdir -p gau_results
          
          # Set PATH
          export PATH="$HOME/go/bin:/usr/local/bin:$PATH"
          
          echo "Testing gau installation..."
          which gau || echo "gau not in standard PATH"
          
          echo "Running gau for: $DOMAIN"
          
          if command -v gau >/dev/null 2>&1; then
            timeout 300 gau "$DOMAIN" --threads 3 --subs > gau_results/gau.txt 2>&1 || {
              echo "gau failed or timed out, checking partial results..."
              touch gau_results/gau.txt
            }
          else
            echo "gau not found, creating empty file"
            touch gau_results/gau.txt
          fi
          
          GAU_COUNT=$(wc -l < gau_results/gau.txt)
          echo "GAU found $GAU_COUNT URLs"
          
          # Add some manual URLs as fallback
          echo "https://${DOMAIN}/search?q=test" >> gau_results/gau.txt
          echo "https://${DOMAIN}/page?id=1" >> gau_results/gau.txt
          echo "https://api.${DOMAIN}/v1/users?limit=10" >> gau_results/gau.txt

      - name: Upload gau results
        uses: actions/upload-artifact@v4
        with:
          name: gau-results
          path: gau_results/

  # Job 3: Combine and prepare
  combine-and-prepare:
    needs: [wayback-collection, gau-collection]
    runs-on: ubuntu-latest
    outputs:
      dynamic-urls-count: ${{ steps.count.outputs.dynamic }}
      total-urls-count: ${{ steps.count.outputs.total }}
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Install required tools
        run: |
          sudo apt update
          sudo apt install -y jq moreutils golang-go curl
          
          # Install Go tools
          export PATH="$HOME/go/bin:$PATH"
          go install github.com/tomnomnom/anew@latest
          go install github.com/tomnomnom/unfurl@latest
          
          # Copy to system bin
          sudo cp $HOME/go/bin/* /usr/local/bin/ 2>/dev/null || echo "Tools available in ~/go/bin/"
          echo "$HOME/go/bin" >> $GITHUB_PATH

      - name: Download all URL collections
        uses: actions/download-artifact@v4
        with:
          merge-multiple: true

      - name: Combine and filter URLs
        run: |
          DOMAIN="${{ github.event.inputs.domain }}"
          mkdir -p combined_results
          
          # Set PATH
          export PATH="$HOME/go/bin:/usr/local/bin:$PATH"
          
          # Extensions to filter out
          EXTS='(json|js|css|jpg|jpeg|png|svg|gif|exe|mp4|flv|pdf|doc|webm|wmv|webp|mov|mp3|avi|zip)($|\?)'
          
          echo "Combining all URLs..."
          
          # Initialize empty file
          touch combined_results/all-raw-urls.txt
          
          # Combine available results
          find . -name "waybackurls.txt" -exec cat {} >> combined_results/all-raw-urls.txt \;
          find . -name "gau.txt" -exec cat {} >> combined_results/all-raw-urls.txt \;
          
          # Add domain root if not present
          echo "https://${DOMAIN}/" >> combined_results/all-raw-urls.txt
          echo "http://${DOMAIN}/" >> combined_results/all-raw-urls.txt
          
          # Add some common paths with parameters for testing
          cat >> combined_results/all-raw-urls.txt << EOF
          https://${DOMAIN}/search?q=test&category=all
          https://${DOMAIN}/login?redirect=%2Fdashboard
          https://${DOMAIN}/api/users?id=1&format=json
          https://${DOMAIN}/product?id=123&color=red
          https://www.${DOMAIN}/page?ref=home&utm_source=google
          https://${DOMAIN}/contact?subject=hello&name=test
          EOF
          
          # Remove empty lines and make unique
          sed '/^$/d' combined_results/all-raw-urls.txt | sort -u > combined_results/temp-urls.txt
          mv combined_results/temp-urls.txt combined_results/all-raw-urls.txt
          
          # Filter extensions
          grep -iEv "\.${EXTS}" combined_results/all-raw-urls.txt > combined_results/all-clean-urls.txt || cp combined_results/all-raw-urls.txt combined_results/all-clean-urls.txt
          
          # Separate static and dynamic
          grep -v '?' combined_results/all-clean-urls.txt > combined_results/static-urls.txt 2>/dev/null || touch combined_results/static-urls.txt
          grep '?' combined_results/all-clean-urls.txt > combined_results/dynamic-urls.txt 2>/dev/null || touch combined_results/dynamic-urls.txt
          
          # Extract parameters
          if [ -s combined_results/all-clean-urls.txt ]; then
            if command -v unfurl >/dev/null 2>&1; then
              unfurl --unique keys < combined_results/all-clean-urls.txt | sort -u > combined_results/params.txt
            else
              # Manual parameter extraction as fallback
              grep '?' combined_results/all-clean-urls.txt | sed 's/.*?//; s/&/
/g' | sed 's/=.*//' | sort -u > combined_results/params.txt
            fi
          else
            touch combined_results/params.txt
          fi
          
          # Add common parameters if file is empty/small
          if [ $(wc -l < combined_results/params.txt) -lt 5 ]; then
            cat >> combined_results/params.txt << EOF
          id
          q
          search
          query
          page
          limit
          offset
          sort
          filter
          category
          name
          email
          user
          redirect
          url
          callback
          ref
          source
          utm_source
          debug
          test
          EOF
            sort -u combined_results/params.txt -o combined_results/params.txt
          fi
          
          echo "URL Summary:"
          echo "- Total raw URLs: $(wc -l < combined_results/all-raw-urls.txt)"
          echo "- Clean URLs: $(wc -l < combined_results/all-clean-urls.txt)"
          echo "- Static URLs: $(wc -l < combined_results/static-urls.txt)"
          echo "- Dynamic URLs: $(wc -l < combined_results/dynamic-urls.txt)"
          echo "- Unique params: $(wc -l < combined_results/params.txt)"
          
          # Show samples
          echo "Sample dynamic URLs:"
          head -3 combined_results/dynamic-urls.txt || echo "No dynamic URLs"
          echo "Sample parameters:"
          head -5 combined_results/params.txt || echo "No parameters"

      - name: Count URLs for matrix
        id: count
        run: |
          DYNAMIC=$(wc -l < combined_results/dynamic-urls.txt)
          TOTAL=$(wc -l < combined_results/all-clean-urls.txt)
          echo "dynamic=$DYNAMIC" >> $GITHUB_OUTPUT
          echo "total=$TOTAL" >> $GITHUB_OUTPUT
          echo "Found $DYNAMIC dynamic URLs, $TOTAL total URLs"

      - name: Upload combined results
        uses: actions/upload-artifact@v4
        with:
          name: combined-urls
          path: combined_results/

  # Job 4: httpx scan (distributed)
  httpx-scan:
    needs: combine-and-prepare
    if: needs.combine-and-prepare.outputs.dynamic-urls-count > 0
    runs-on: ubuntu-latest
    strategy:
      matrix:
        worker: [1, 2, 3, 4, 5]
    
    steps:
      - name: Install httpx
        run: |
          sudo apt update && sudo apt install -y golang-go
          
          export PATH="$HOME/go/bin:$PATH"
          go install -v github.com/projectdiscovery/httpx/cmd/httpx@latest
          
          sudo cp $HOME/go/bin/httpx /usr/local/bin/ 2>/dev/null || echo "httpx in ~/go/bin/"
          echo "$HOME/go/bin" >> $GITHUB_PATH

      - name: Download combined URLs
        uses: actions/download-artifact@v4
        with:
          name: combined-urls

      - name: Run httpx on chunk
        run: |
          WORKER=${{ matrix.worker }}
          TOTAL_WORKERS=5
          
          mkdir -p httpx_worker_${WORKER}
          export PATH="$HOME/go/bin:/usr/local/bin:$PATH"
          
          if [ ! -f "combined_results/dynamic-urls.txt" ] || [ ! -s "combined_results/dynamic-urls.txt" ]; then
            echo "No dynamic URLs for worker $WORKER"
            touch httpx_worker_${WORKER}/empty
            exit 0
          fi
          
          # Split URLs
          TOTAL_URLS=$(wc -l < combined_results/dynamic-urls.txt)
          CHUNK_SIZE=$((TOTAL_URLS / TOTAL_WORKERS + 1))
          START_LINE=$(((WORKER - 1) * CHUNK_SIZE + 1))
          END_LINE=$((WORKER * CHUNK_SIZE))
          
          echo "Worker $WORKER: lines $START_LINE-$END_LINE from $TOTAL_URLS total"
          
          sed -n "${START_LINE},${END_LINE}p" combined_results/dynamic-urls.txt > httpx_worker_${WORKER}/chunk.txt
          WORKER_URLS=$(wc -l < httpx_worker_${WORKER}/chunk.txt)
          
          if [ "$WORKER_URLS" -eq 0 ]; then
            touch httpx_worker_${WORKER}/empty
            exit 0
          fi
          
          echo "Worker $WORKER processing $WORKER_URLS URLs"
          
          # Run httpx
          if command -v httpx >/dev/null 2>&1; then
            timeout 180 httpx -l httpx_worker_${WORKER}/chunk.txt \
              -silent -threads 30 -timeout 5 -retries 1 \
              -status-code -follow-redirects \
              > httpx_worker_${WORKER}/alive.txt 2>/dev/null || touch httpx_worker_${WORKER}/alive.txt
          else
            echo "httpx not found, using all URLs as alive"
            cp httpx_worker_${WORKER}/chunk.txt httpx_worker_${WORKER}/alive.txt
          fi
          
          ALIVE_COUNT=$(wc -l < httpx_worker_${WORKER}/alive.txt)
          echo "Worker $WORKER found $ALIVE_COUNT alive URLs"

      - name: Upload httpx results
        uses: actions/upload-artifact@v4
        with:
          name: httpx-worker-${{ matrix.worker }}
          path: httpx_worker_${{ matrix.worker }}/

  # Job 5: Final summary (simplified, skip complex tools for now)
  final-summary:
    needs: [combine-and-prepare, httpx-scan]
    if: always()
    runs-on: ubuntu-latest
    
    steps:
      - name: Download all results
        uses: actions/download-artifact@v4
        with:
          merge-multiple: true

      - name: Generate final report
        run: |
          DOMAIN="${{ github.event.inputs.domain }}"
          mkdir -p final_results
          
          # Combine httpx results
          find . -name "alive.txt" -exec cat {} \; | sort -u > final_results/all-alive-urls.txt 2>/dev/null || touch final_results/all-alive-urls.txt
          
          # Create empty files for now (will add x8/kxss in next iteration)
          touch final_results/all-x8-reflections.txt
          touch final_results/all-kxss-reflections.txt
          touch final_results/all-x8-raw.txt
          touch final_results/all-kxss-raw.txt
          
          # Copy URL data
          if [ -d "combined_results" ]; then
            cp combined_results/* final_results/ 2>/dev/null || true
          fi
          
          # Generate summary
          TOTAL_URLS=$(wc -l < final_results/all-clean-urls.txt 2>/dev/null || echo 0)
          DYNAMIC_URLS=$(wc -l < final_results/dynamic-urls.txt 2>/dev/null || echo 0)
          ALIVE_URLS=$(wc -l < final_results/all-alive-urls.txt 2>/dev/null || echo 0)
          PARAMS=$(wc -l < final_results/params.txt 2>/dev/null || echo 0)
          
          cat > final_results/FINAL_SUMMARY.txt << EOF
          ==================== DISTRIBUTED SCAN RESULTS ====================
          Domain: $DOMAIN
          Date: $(date)
          Scan Status: URL Collection and Alive Check Completed
          
          📊 URL Statistics:
          - Total URLs found: $TOTAL_URLS
          - Dynamic URLs: $DYNAMIC_URLS  
          - Alive URLs (httpx): $ALIVE_URLS
          - Unique parameters: $PARAMS
          
          🔍 Reflection Results:
          - X8 reflections: 0 (skipped in this run)
          - KXSS reflections: 0 (skipped in this run)
          - Status: Basic URL collection successful
          
          📁 Available Files:
          - all-alive-urls.txt: Live URLs after httpx verification
          - all-clean-urls.txt: Filtered URL collection
          - dynamic-urls.txt: URLs with parameters
          - params.txt: Unique parameter names found
          
          Next Steps: Run advanced reflection testing with working tools
          ==================================================================
          EOF
          
          echo "========== FINAL SUMMARY =========="
          cat final_results/FINAL_SUMMARY.txt
          
          # Show samples
          echo ""
          echo "📋 Sample Results:"
          if [ "$ALIVE_URLS" -gt 0 ]; then
            echo "🟢 Top 5 Alive URLs:"
            head -5 final_results/all-alive-urls.txt
          fi
          
          if [ "$PARAMS" -gt 0 ]; then
            echo ""
            echo "🔧 Top 10 Parameters:"
            head -10 final_results/params.txt
          fi

      - name: Upload final results
        uses: actions/upload-artifact@v4
        with:
          name: FINAL-RESULTS-${{ github.event.inputs.domain }}
          path: final_results/
